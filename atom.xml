<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>welcome</title>
  
  <subtitle>Tensor Robotics</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.tensor-robotics.com/"/>
  <updated>2020-03-22T14:06:03.186Z</updated>
  <id>http://blog.tensor-robotics.com/</id>
  
  <author>
    <name>chopin1998@gmail.com</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于ToF 番四</title>
    <link href="http://blog.tensor-robotics.com/2020/03/22/tof/tof-4/"/>
    <id>http://blog.tensor-robotics.com/2020/03/22/tof/tof-4/</id>
    <published>2020-03-22T08:51:06.000Z</published>
    <updated>2020-03-22T14:06:03.186Z</updated>
    
    <content type="html"><![CDATA[<p>由于各种原因，和本番相关的工作被严重耽误……以至于错过大好机会</p><p><strong>痛定思痛 痛何如哉</strong></p><hr><p>本番行文后，将在另外一种形式上重启</p><hr><p>在 【关于ToF 番二】 中介绍的ToF系统，使用连续调制光照射外界，并通过多帧数据，计算得出反射光的相位偏移角度， 从而间接计算出光线行走距离。</p><p>但，这不是唯一的方式，本文介绍另外一种ToF相机的操作原理：脉冲式ToF</p><p>让我们来构建一个和连续调制方式ToF的有点儿相似但不完全相同的像素结构</p><p><img src="pulse_tof_gate.webp" alt=""></p><ul><li>首先操作时， chg会打开，对两侧两颗电容充电</li><li>然后S0(或S1)进行开关操作，开关时机由调制模块控制</li><li>像素点开始曝光，并将光信号转变成电荷</li><li>当S0（或S1）闭合时，积分电荷就会和对应电容中和</li><li>最后读取电容中的剩余电压</li></ul><p>然后有下图</p><p><img src="op.webp" alt=""></p><ul><li>第一行 代表受控的LED输出，正是因为这个，才被称之为脉冲式</li><li>第二行 代表实际的光线反射回波</li></ul><hr><p>LED发射的每个脉冲长度为 t， 而S0和S1的采样时长也为t，但是S0于LED同时开启，S1在S0关闭后立即打开。因此得到第三行和第四行</p><ul><li>第三行 是S0的积分</li><li>第四行 是S1的积分</li></ul><p>得到公式</p><p> d = 1/2 * C * t * (S0) / (S0 + S1)</p><p>可以看到，和连续调制ToF相比</p><ul><li>脉冲ToF的结算会更容易。当然二者基本还是在一个水平线上，一个是加减乘除，一个带三角函数</li><li>连续调制ToF的像素点因为是差分结构，对环境光噪音的抑制有天生的优势</li><li>听说脉冲式ToF的精度会略高，当然还要看各自厂家的具体优化手段</li><li>连续调制ToF在计算中，可以得到一些附结论（比如误差估计）</li></ul><hr><p>最后附上一种连续调制式ToF的像素结构图</p><p><img src="tune.jpeg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于各种原因，和本番相关的工作被严重耽误……以至于错过大好机会&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;痛定思痛 痛何如哉&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;本番行文后，将在另外一种形式上重启&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;在 【关于ToF 番二】 中介绍的ToF系统，使用连续调制
      
    
    </summary>
    
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/categories/ToF/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/tags/ToF/"/>
    
  </entry>
  
  <entry>
    <title>关于ToF 番三</title>
    <link href="http://blog.tensor-robotics.com/2020/03/22/tof/tof-3/"/>
    <id>http://blog.tensor-robotics.com/2020/03/22/tof/tof-3/</id>
    <published>2020-03-22T08:03:06.000Z</published>
    <updated>2020-03-22T14:06:09.659Z</updated>
    
    <content type="html"><![CDATA[<p>由于各种事物影响， ToF项目被暂缓了￼， 趁着公司再次搬家的这段时间，重新捡起来。</p><iframe frameborder="0" src="https://v.qq.com/txp/iframe/player.html?vid=o1357dpu781" allowfullscreen="true"></iframe><p>先是把代码又理了一遍，抽空学习了一下numpy， 搞一点儿opencv，tensorflow和pytorch，把第二编辑器换成了VSCode（顺便看了一下markdown和简单的Latex语法），换用Python 3.7，还有vrep， openscad，steamVR HDK什么的……咦，好像扯远了……然后继续攻目前的障碍。这把终于弄通了￼ 其实很简单的嘛。。。。</p><hr><p>非英语母语国家的厂家写的英文文档遇上了半吊子英语水平的我的结果是世界上最远的距离变成了本来往后退一步就能到达的地方却要拼命往前跑￼</p><p>是的，我看文档时就和你读上句话的感受是一样的￼</p><p><img src="blurred.webp" alt=""><br> 这是我用mardkdown做的笔记， 数学公式真的好漂亮。有多漂亮呢？为了让人看不清楚，我特地使用了高斯模糊处理了一下</p><hr><p>前面两番已经交代了深度视觉特别是ToF技术的概况，今天来谈一谈ToF具体实现上的大坑（之一，我预感我还会面临最后一个坑）。</p><p>ToF是一把用来测量距离的尺子，造一把尺子的关注点是精度，而造一批尺子关注的是制造精度的分布。</p><hr><p>一把尺子造出来， 如何知道它是准的？有多么不准？不准了怎么办？￼</p><p>这就涉及到标定 &amp; 补偿</p><ul><li>标定，是指出厂前，使用另外一把更准的尺子教新尺子重新做人，标定后会得到一组标定数据</li><li>补偿，是在运行时，利用出厂前的标定数据，实时地对当前原始数据进行算法处理，得到最终的结果</li></ul><p>那么为什么从传感器直接得到的原始数据不能拿来用呢？我也想呀！可我们面对的是裸片，是真实的物理世界，搞不好还有量子效应。</p><p>因为NDA的缘故， 我不能说得太具体￼ 但参考各家公开的文档，基本情况大差不差。</p><p>造成ToF系统产生误差的重要原因是“热”  中医分内热和外热两种，电烙铁也是如此… 热导致电信号（电子、空穴移动）在硅基芯片内的传导时间发生变化，而ToF高度依赖时间测量的精度。 光速大约 30厘米/纳秒， 若要达到厘米级的测距精度，对时间的测量精度不能低于30皮秒级别￼</p><p>然而环境温度的变化， 芯片工作时的温升， 都会导致精度的大规模失准， 特别是像素和像素之间的温差和其它工艺因素的差别会更加劣化这种情况￼</p><p>不同厂家针对器件的特性，都有各自的办法来解决这个问题。有些厂家的的设计比较独到，为批量化自动生产打开了便利之门。</p><p>另外一个造成误差的主要因素是环境光， 虽说相位检测的ToF系统天生具有差分读取结构，但是过强的环境光或者环境光扰动会使得调制信号的信噪比降低。 </p><p>对于这种情况，补偿算法是一方面，加强调制光照明强度，减少曝光时间，以及光学滤镜都是可以尝试的手段。</p><p>当然还有一些其它的因素，有些是特定器件相关的， 有些影响相对较小，这里不再一一熬述￼</p><p>听上去有点儿复杂，实现起来细节也确实很多，但 是可行的。</p><p>我想这就是个机会，将来总会有厂家把这些算法ASIC化，或者有人给出更简单的打包方案，不过目前，我将会来完成这些￼</p><p>毕竟朕是在暴风雨里单日骑行超过400km的人， 老子的目标在很远的地方。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于各种事物影响， ToF项目被暂缓了￼， 趁着公司再次搬家的这段时间，重新捡起来。&lt;/p&gt;
&lt;iframe frameborder=&quot;0&quot; src=&quot;https://v.qq.com/txp/iframe/player.html?vid=o1357dpu781&quot; allo
      
    
    </summary>
    
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/categories/ToF/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/tags/ToF/"/>
    
  </entry>
  
  <entry>
    <title>关于ToF 番二</title>
    <link href="http://blog.tensor-robotics.com/2020/03/22/tof/tof-2/"/>
    <id>http://blog.tensor-robotics.com/2020/03/22/tof/tof-2/</id>
    <published>2020-03-22T07:19:30.000Z</published>
    <updated>2020-03-22T14:06:12.186Z</updated>
    
    <content type="html"><![CDATA[<p>书接前文。【关于ToF 番一】</p><p>上回说到ToF的基本概念， 即LED/LD对着场景发射光线，如果场景中有物体反射光线回来，那么我们测定这道光线从发射到被接收的时间间隔， 就能计算出距该离物体的长度为 1/2 * c * t， c是光速。</p><p>举个例子， 如果光线的旅行时间测得为 5ns， 取光速为30万千米每秒， 那么光线总的行走距离就是 300000000 * 0.000000005 = 1.5米， 因为是一来一回再除以二， 就知道有个物体距离接收器0.75米（光源和接收器在同一位置）</p><p><img src="measure.png" alt=""></p><hr><p>对于一个直接ToF系统来说，精度和误差均只依赖于对时间的测量精度。</p><p>当然，由于目前的技术限制，使用直接ToF的成本相对较高，单个diode的体积也很大，一般只用于单点或者几个点的方案。对于阵列式的图像ToF系统，使用的是间接ToF方式。 即不直接测量光线往返时间， 而是发射连续的调制光（例如正玄波）， 通过测量相位偏移的方式间接得出ToF。</p><p>初始发射时从正玄波的0度角开始连续发射， 然后接收到光线时， 通过鉴相算法， 得到相位，通过这个相位差以及调制光的频率， 就能算出光线的飞行时间。 真TM高级！ 不过这也引入了一个雷达里的常用术语：maximum unambiguous range。 具体的算法以及处理，本文暂且按下不表，日后分解，下同。</p><p><img src="phase.png" alt=""></p><hr><p>虽然降低了难度， 但是也引入更多噪音。 因为测量相移受到诸多因素的影响，例如系统时钟的ppm，传感器温度，物体反射强度，背景光线强度等。</p><p>这回我们就简单说说ToF的工作、设计流程。</p><p>从整个系统看， 首先要考虑的是测距范围问题。 因为光线强度的减弱是距离倒数的平方关系。 例如：期待的测距范围是 5cm ~ 5m， 假如在5cm时的照明输出强度是1的话， 满足5m处同样条件的输出强度得是1000。当然合理的设计光路和LED位置布局， 可以在一定程度上减少功率输出的需求。</p><p>而如果被侧物体中有大量黑体，噗… 是对红外反射率较低的黑色物体， 那么对发射功率的要求又会提高。</p><p>ToF阵列里的每个pixel， 都可以计算出它的振幅质量， 如果过低（欠曝）或者过高（过曝）都无法解析出距离信息， 需要在下一个周期中，进行曝光调整。</p><p>如果需要更高的动态范围， 可以使用各类HDR技术，例如多帧曝光、行间切换曝光等。</p><p>对于曝光时间的选择，还有另外一个问题需要考量，曝光时间越长，就会有越多的环境光被吸收， 这会伤害整体精度（环境光的变化？）。 同时过长的曝光时间， 也会造成帧率降低。 一般曝光时间不要超过1ms，或者可以使用带通滤光片过滤掉调制光频率以外的光线。</p><p>一个典型的ToF计算流程包括：</p><ul><li>多次以不同时长曝光的ToF数据采样（用于鉴相，取得距离数据）</li><li>一次最小曝光时间的灰度数据采集（用于过滤环境光线）</li><li>周期性的传感器片上温度数据读取（用于温度补偿)</li><li>根据算法取出有效数据（过滤掉过曝太多、欠曝过多或者振幅数值过小的数据集）</li><li>温度补偿算法</li><li>背景光补偿算法</li><li>系统常量校准算法</li></ul><hr><p>to be continued</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;书接前文。【关于ToF 番一】&lt;/p&gt;
&lt;p&gt;上回说到ToF的基本概念， 即LED/LD对着场景发射光线，如果场景中有物体反射光线回来，那么我们测定这道光线从发射到被接收的时间间隔， 就能计算出距该离物体的长度为 1/2 * c * t， c是光速。&lt;/p&gt;
&lt;p&gt;举个例子
      
    
    </summary>
    
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/categories/ToF/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/tags/ToF/"/>
    
  </entry>
  
  <entry>
    <title>关于ToF 番一</title>
    <link href="http://blog.tensor-robotics.com/2020/03/22/tof/tof-1/"/>
    <id>http://blog.tensor-robotics.com/2020/03/22/tof/tof-1/</id>
    <published>2020-03-22T06:32:01.000Z</published>
    <updated>2020-03-22T14:06:06.266Z</updated>
    
    <content type="html"><![CDATA[<h2 id="曾经，通过CCD-CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。"><a href="#曾经，通过CCD-CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。" class="headerlink" title="曾经，通过CCD/CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。"></a>曾经，通过CCD/CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。</h2><h2 id="现在，日新月异的传感器技术以及强大的移动计算能力，让计算机获得深度感知能力的可能性也越来越多。"><a href="#现在，日新月异的传感器技术以及强大的移动计算能力，让计算机获得深度感知能力的可能性也越来越多。" class="headerlink" title="现在，日新月异的传感器技术以及强大的移动计算能力，让计算机获得深度感知能力的可能性也越来越多。"></a>现在，日新月异的传感器技术以及强大的移动计算能力，让计算机获得深度感知能力的可能性也越来越多。</h2><h2 id="当获得一帧3D深度图景时，会收到一幅2D深度阵列，-其中的每个”pixel”都代表了该点对应物体的实际距离。闭上眼睛想象一下，那是一堆密密麻麻的点云。机器人将更容易的进行自主定位，计算机能更轻易的识别目标，也许新的测量工具，新的交互方式也会应运而生。"><a href="#当获得一帧3D深度图景时，会收到一幅2D深度阵列，-其中的每个”pixel”都代表了该点对应物体的实际距离。闭上眼睛想象一下，那是一堆密密麻麻的点云。机器人将更容易的进行自主定位，计算机能更轻易的识别目标，也许新的测量工具，新的交互方式也会应运而生。" class="headerlink" title="当获得一帧3D深度图景时，会收到一幅2D深度阵列， 其中的每个”pixel”都代表了该点对应物体的实际距离。闭上眼睛想象一下，那是一堆密密麻麻的点云。机器人将更容易的进行自主定位，计算机能更轻易的识别目标，也许新的测量工具，新的交互方式也会应运而生。"></a>当获得一帧3D深度图景时，会收到一幅2D深度阵列， 其中的每个”pixel”都代表了该点对应物体的实际距离。闭上眼睛想象一下，那是一堆密密麻麻的点云。机器人将更容易的进行自主定位，计算机能更轻易的识别目标，也许新的测量工具，新的交互方式也会应运而生。</h2><h2 id="双眼（和大脑）是人类获得深度感知最自然的渠道。通过长在不同地方的两只眼睛￼，视网膜上被投影了两帧“略有差异”的倒影，大脑根据几十年来的习得，分析出这两个影像中相同的物体，以及它们距离我们自身的大致距离。"><a href="#双眼（和大脑）是人类获得深度感知最自然的渠道。通过长在不同地方的两只眼睛￼，视网膜上被投影了两帧“略有差异”的倒影，大脑根据几十年来的习得，分析出这两个影像中相同的物体，以及它们距离我们自身的大致距离。" class="headerlink" title="双眼（和大脑）是人类获得深度感知最自然的渠道。通过长在不同地方的两只眼睛￼，视网膜上被投影了两帧“略有差异”的倒影，大脑根据几十年来的习得，分析出这两个影像中相同的物体，以及它们距离我们自身的大致距离。"></a>双眼（和大脑）是人类获得深度感知最自然的渠道。通过长在不同地方的两只眼睛￼，视网膜上被投影了两帧“略有差异”的倒影，大脑根据几十年来的习得，分析出这两个影像中相同的物体，以及它们距离我们自身的大致距离。</h2><h2 id="这个原理也是计算机目前深度视觉原理的三大流派之一：-双目视觉。-另外两个技术流，一个基于ToF测距技术，-另外一个基于Structured-Light技术。当然这是主要的分类，具体场景，具体产品，可能会相互融合，或者辅以其它技术、算法。"><a href="#这个原理也是计算机目前深度视觉原理的三大流派之一：-双目视觉。-另外两个技术流，一个基于ToF测距技术，-另外一个基于Structured-Light技术。当然这是主要的分类，具体场景，具体产品，可能会相互融合，或者辅以其它技术、算法。" class="headerlink" title="这个原理也是计算机目前深度视觉原理的三大流派之一： 双目视觉。 另外两个技术流，一个基于ToF测距技术， 另外一个基于Structured Light技术。当然这是主要的分类，具体场景，具体产品，可能会相互融合，或者辅以其它技术、算法。"></a>这个原理也是计算机目前深度视觉原理的三大流派之一： 双目视觉。 另外两个技术流，一个基于ToF测距技术， 另外一个基于Structured Light技术。当然这是主要的分类，具体场景，具体产品，可能会相互融合，或者辅以其它技术、算法。</h2><h2 id="此番将大致比较一下这三个技术流的主要特征。-下一更将重点介绍本屌目前正在玩儿的ToF方案原理。"><a href="#此番将大致比较一下这三个技术流的主要特征。-下一更将重点介绍本屌目前正在玩儿的ToF方案原理。" class="headerlink" title="此番将大致比较一下这三个技术流的主要特征。 下一更将重点介绍本屌目前正在玩儿的ToF方案原理。"></a>此番将大致比较一下这三个技术流的主要特征。 下一更将重点介绍本屌目前正在玩儿的ToF方案原理。</h2><hr><h3 id="就双目视觉来说，-对前端硬件特殊性的要求很低，两个普通摄像头即可（或许有同步采样要求）。但要从两张平面图像恢复出原本的深度信息，重建工作对于算力、算法的要求可是相当相当的高。此外对于弱光环境、光线扰动、无特征的面（一堵白墙）都无法很好处理。其速度响应、帧率、可靠性都会多少有问题。至于它的优点嘛。。除了刚刚说的硬件比较便宜外，还有一个就是分辨率理论上可以做得比较高（受限于算力）。"><a href="#就双目视觉来说，-对前端硬件特殊性的要求很低，两个普通摄像头即可（或许有同步采样要求）。但要从两张平面图像恢复出原本的深度信息，重建工作对于算力、算法的要求可是相当相当的高。此外对于弱光环境、光线扰动、无特征的面（一堵白墙）都无法很好处理。其速度响应、帧率、可靠性都会多少有问题。至于它的优点嘛。。除了刚刚说的硬件比较便宜外，还有一个就是分辨率理论上可以做得比较高（受限于算力）。" class="headerlink" title="就双目视觉来说， 对前端硬件特殊性的要求很低，两个普通摄像头即可（或许有同步采样要求）。但要从两张平面图像恢复出原本的深度信息，重建工作对于算力、算法的要求可是相当相当的高。此外对于弱光环境、光线扰动、无特征的面（一堵白墙）都无法很好处理。其速度响应、帧率、可靠性都会多少有问题。至于它的优点嘛。。除了刚刚说的硬件比较便宜外，还有一个就是分辨率理论上可以做得比较高（受限于算力）。"></a>就双目视觉来说， 对前端硬件特殊性的要求很低，两个普通摄像头即可（或许有同步采样要求）。但要从两张平面图像恢复出原本的深度信息，重建工作对于算力、算法的要求可是相当相当的高。此外对于弱光环境、光线扰动、无特征的面（一堵白墙）都无法很好处理。其速度响应、帧率、可靠性都会多少有问题。至于它的优点嘛。。除了刚刚说的硬件比较便宜外，还有一个就是分辨率理论上可以做得比较高（受限于算力）。</h3><h3 id="而Structured-Light呢，翻译过来是“结构光”。-是一种很好玩儿的技术。-最新的Apple-iPhone-X应该就是这个技术的一个变种。"><a href="#而Structured-Light呢，翻译过来是“结构光”。-是一种很好玩儿的技术。-最新的Apple-iPhone-X应该就是这个技术的一个变种。" class="headerlink" title="而Structured Light呢，翻译过来是“结构光”。 是一种很好玩儿的技术。 最新的Apple iPhone X应该就是这个技术的一个变种。"></a>而Structured Light呢，翻译过来是“结构光”。 是一种很好玩儿的技术。 最新的Apple iPhone X应该就是这个技术的一个变种。</h3><h3 id="基于结构光技术的深度系统，有一个主动（红外）光源，像外界投射设计过的点阵，或者干涉条纹或者别的什么pattern。这些pattern光遇到物体反射回摄像头。后端算法会根据这些点在摄像头里的位置推出物体实际的距离。"><a href="#基于结构光技术的深度系统，有一个主动（红外）光源，像外界投射设计过的点阵，或者干涉条纹或者别的什么pattern。这些pattern光遇到物体反射回摄像头。后端算法会根据这些点在摄像头里的位置推出物体实际的距离。" class="headerlink" title="基于结构光技术的深度系统，有一个主动（红外）光源，像外界投射设计过的点阵，或者干涉条纹或者别的什么pattern。这些pattern光遇到物体反射回摄像头。后端算法会根据这些点在摄像头里的位置推出物体实际的距离。"></a>基于结构光技术的深度系统，有一个主动（红外）光源，像外界投射设计过的点阵，或者干涉条纹或者别的什么pattern。这些pattern光遇到物体反射回摄像头。后端算法会根据这些点在摄像头里的位置推出物体实际的距离。</h3><h3 id="结构光技术对算法、算力的要求比起双面视觉来说下降很多。因为是主动投射，对弱光环境下的效果显著提升。但因为需要投影pattern，对超强光下的应用反而是受限的。此外结构光技术的深度系统，只能测定极短-中短距离的物体，搞不远。。。分辨精度在mm-cm级别。"><a href="#结构光技术对算法、算力的要求比起双面视觉来说下降很多。因为是主动投射，对弱光环境下的效果显著提升。但因为需要投影pattern，对超强光下的应用反而是受限的。此外结构光技术的深度系统，只能测定极短-中短距离的物体，搞不远。。。分辨精度在mm-cm级别。" class="headerlink" title="结构光技术对算法、算力的要求比起双面视觉来说下降很多。因为是主动投射，对弱光环境下的效果显著提升。但因为需要投影pattern，对超强光下的应用反而是受限的。此外结构光技术的深度系统，只能测定极短-中短距离的物体，搞不远。。。分辨精度在mm-cm级别。"></a>结构光技术对算法、算力的要求比起双面视觉来说下降很多。因为是主动投射，对弱光环境下的效果显著提升。但因为需要投影pattern，对超强光下的应用反而是受限的。此外结构光技术的深度系统，只能测定极短-中短距离的物体，搞不远。。。分辨精度在mm-cm级别。</h3><h3 id="intel的realsense和老款m-Kinect是代表产品。"><a href="#intel的realsense和老款m-Kinect是代表产品。" class="headerlink" title="intel的realsense和老款m$ Kinect是代表产品。"></a>intel的realsense和老款m$ Kinect是代表产品。</h3><h3 id="传统的固定pattern投影技术，深度分辨率也受到pattern发生器的限制。"><a href="#传统的固定pattern投影技术，深度分辨率也受到pattern发生器的限制。" class="headerlink" title="传统的固定pattern投影技术，深度分辨率也受到pattern发生器的限制。"></a>传统的固定pattern投影技术，深度分辨率也受到pattern发生器的限制。</h3><h3 id="苹果那个就比较牛逼，传说pattern是可变的，分辨精度在极短距离内可以达到惊人的um级别，分辨率在适当的范围内也相对喜人。"><a href="#苹果那个就比较牛逼，传说pattern是可变的，分辨精度在极短距离内可以达到惊人的um级别，分辨率在适当的范围内也相对喜人。" class="headerlink" title="苹果那个就比较牛逼，传说pattern是可变的，分辨精度在极短距离内可以达到惊人的um级别，分辨率在适当的范围内也相对喜人。"></a>苹果那个就比较牛逼，传说pattern是可变的，分辨精度在极短距离内可以达到惊人的um级别，分辨率在适当的范围内也相对喜人。</h3><h3 id="ToF的全称是”Time-of-Flight”。本意是通过测量光线的折返时间来算出距离。我们都知道-距离-速度-×-时间。光速爱因斯坦告诉我们不变了，现在只要测定光线从物体反射回来的时间就能很容易算出距离来了。听上去是不是很酷！要测定一秒绕地球七圈的光，行走很短距离的时间？具体原理和操作，下一弹说￼。"><a href="#ToF的全称是”Time-of-Flight”。本意是通过测量光线的折返时间来算出距离。我们都知道-距离-速度-×-时间。光速爱因斯坦告诉我们不变了，现在只要测定光线从物体反射回来的时间就能很容易算出距离来了。听上去是不是很酷！要测定一秒绕地球七圈的光，行走很短距离的时间？具体原理和操作，下一弹说￼。" class="headerlink" title="ToF的全称是”Time of Flight”。本意是通过测量光线的折返时间来算出距离。我们都知道 距离 = 速度 × 时间。光速爱因斯坦告诉我们不变了，现在只要测定光线从物体反射回来的时间就能很容易算出距离来了。听上去是不是很酷！要测定一秒绕地球七圈的光，行走很短距离的时间？具体原理和操作，下一弹说￼。"></a>ToF的全称是”Time of Flight”。本意是通过测量光线的折返时间来算出距离。我们都知道 距离 = 速度 × 时间。光速爱因斯坦告诉我们不变了，现在只要测定光线从物体反射回来的时间就能很容易算出距离来了。听上去是不是很酷！要测定一秒绕地球七圈的光，行走很短距离的时间？具体原理和操作，下一弹说￼。</h3><h3 id="和结构光一样，ToF同样需要主动发射光线。当然不是“结构”的，它只是亮和暗，视界内全面覆盖。所以功耗会比结构光的大一丢丢，特别是远距离的时候，喔，对了反正结构光也搞不远…"><a href="#和结构光一样，ToF同样需要主动发射光线。当然不是“结构”的，它只是亮和暗，视界内全面覆盖。所以功耗会比结构光的大一丢丢，特别是远距离的时候，喔，对了反正结构光也搞不远…" class="headerlink" title="和结构光一样，ToF同样需要主动发射光线。当然不是“结构”的，它只是亮和暗，视界内全面覆盖。所以功耗会比结构光的大一丢丢，特别是远距离的时候，喔，对了反正结构光也搞不远…"></a>和结构光一样，ToF同样需要主动发射光线。当然不是“结构”的，它只是亮和暗，视界内全面覆盖。所以功耗会比结构光的大一丢丢，特别是远距离的时候，喔，对了反正结构光也搞不远…</h3><h3 id="新款m-Kinect是这个架构的代表。"><a href="#新款m-Kinect是这个架构的代表。" class="headerlink" title="新款m$ Kinect是这个架构的代表。"></a>新款m$ Kinect是这个架构的代表。</h3><h3 id="ToF技术架构的优势之一就是计算相对最简单，几乎是传感器直接测量。帧率高并因此响应极快。抗外界光线干扰能力很强。长、短距离皆可应用。分辨精度在mm～cm级别之间。"><a href="#ToF技术架构的优势之一就是计算相对最简单，几乎是传感器直接测量。帧率高并因此响应极快。抗外界光线干扰能力很强。长、短距离皆可应用。分辨精度在mm～cm级别之间。" class="headerlink" title="ToF技术架构的优势之一就是计算相对最简单，几乎是传感器直接测量。帧率高并因此响应极快。抗外界光线干扰能力很强。长、短距离皆可应用。分辨精度在mm～cm级别之间。"></a>ToF技术架构的优势之一就是计算相对最简单，几乎是传感器直接测量。帧率高并因此响应极快。抗外界光线干扰能力很强。长、短距离皆可应用。分辨精度在mm～cm级别之间。</h3><h3 id="缺陷除了刚刚提到的功耗外，分辨率是最大硬伤。目前-320x240算旗舰，640x480等更高规格器件或许在路线图上"><a href="#缺陷除了刚刚提到的功耗外，分辨率是最大硬伤。目前-320x240算旗舰，640x480等更高规格器件或许在路线图上" class="headerlink" title="缺陷除了刚刚提到的功耗外，分辨率是最大硬伤。目前~320x240算旗舰，640x480等更高规格器件或许在路线图上"></a>缺陷除了刚刚提到的功耗外，分辨率是最大硬伤。目前~320x240算旗舰，640x480等更高规格器件或许在路线图上</h3><h3 id="未来的道路在哪里？谁知道呢。。。走着瞧吧。。。"><a href="#未来的道路在哪里？谁知道呢。。。走着瞧吧。。。" class="headerlink" title="未来的道路在哪里？谁知道呢。。。走着瞧吧。。。"></a>未来的道路在哪里？谁知道呢。。。走着瞧吧。。。</h3><h3 id="客户需求-成本控制-正确的技术路线"><a href="#客户需求-成本控制-正确的技术路线" class="headerlink" title="客户需求 + 成本控制 + 正确的技术路线"></a>客户需求 + 成本控制 + 正确的技术路线</h3><h3 id="听说把握好这三点，就能实现财务自由￼￼￼￼"><a href="#听说把握好这三点，就能实现财务自由￼￼￼￼" class="headerlink" title="听说把握好这三点，就能实现财务自由￼￼￼￼"></a>听说把握好这三点，就能实现财务自由￼￼￼￼</h3><h3 id="而我呢，目前能抓住一个半就好￼"><a href="#而我呢，目前能抓住一个半就好￼" class="headerlink" title="而我呢，目前能抓住一个半就好￼"></a>而我呢，目前能抓住一个半就好￼</h3><h3 id="to-be-continued"><a href="#to-be-continued" class="headerlink" title="to be continued"></a>to be continued</h3><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;曾经，通过CCD-CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。&quot;&gt;&lt;a href=&quot;#曾经，通过CCD-CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。&quot; class=&quot;headerlink&quot; title=&quot;曾经，通过C
      
    
    </summary>
    
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/categories/ToF/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/tags/ToF/"/>
    
  </entry>
  
  <entry>
    <title>Python ctypes的使用实例</title>
    <link href="http://blog.tensor-robotics.com/2020/03/22/python-ctypes-examples/"/>
    <id>http://blog.tensor-robotics.com/2020/03/22/python-ctypes-examples/</id>
    <published>2020-03-22T03:17:56.000Z</published>
    <updated>2020-03-22T03:59:27.926Z</updated>
    
    <content type="html"><![CDATA[<p>Python中可以使用ctypes模块方便的调用C代码写的库， 而无须对C代码有任何多余的要求。</p><p>此仅列一例，权做记录。</p><hr><p><img src="ls.png" alt=""></p><p>首先有两个文件， C和对应的头文件。</p><p>在people.c中， 有三个非常简单的函数。 供日后调用。<br>而在people.h中， 是对应的函数声明， 在此不表。</p><p><img src="source.png" alt=""></p><hr><p>接下来的一步， 将这个C代码编译成库， 使用如下的命令<br><img src="compile.png" alt=""></p><p>这将生成一个叫 libpeople.so的库， 和平常编译库的方式完全一样。</p><hr><p>最后， 激动人心的时刻—-直接在python中调用这个C库。<br><img src="call.png" alt=""></p><p>进入ipython后， 首当其冲import ctypes， 然后用cdll里的LoadLibrary导入刚刚生成的库。</p><p>因为make_people使用了自定义的struct和指针， 所以需要现在Python里先建立对于的数据结构。</p><p>之后就能随心所欲的调用c库里的函数了。</p><hr><p>一些说明。<br>库导入后本身没有动态性， 也就是说在ipython里， 输入完mylib.之后，按tab键是不能自动补全所有的function的，但是手工输入一次之后， 还是能看到已经有的东西。</p><p>因为Python的脚本性， 如果C库没有做好类型检查，特别是指针相关的，而发生致命错误的，在Python动态运行时， Python解析器进程会崩溃。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Python中可以使用ctypes模块方便的调用C代码写的库， 而无须对C代码有任何多余的要求。&lt;/p&gt;
&lt;p&gt;此仅列一例，权做记录。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;ls.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;首先有两个文件， C和对应的头文件。&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
      <category term="CS" scheme="http://blog.tensor-robotics.com/categories/CS/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="Python" scheme="http://blog.tensor-robotics.com/tags/Python/"/>
    
      <category term="ctypes" scheme="http://blog.tensor-robotics.com/tags/ctypes/"/>
    
  </entry>
  
  <entry>
    <title>the first blog</title>
    <link href="http://blog.tensor-robotics.com/2020/03/19/the-first-blog/"/>
    <id>http://blog.tensor-robotics.com/2020/03/19/the-first-blog/</id>
    <published>2020-03-19T12:00:01.000Z</published>
    <updated>2020-03-22T04:01:49.488Z</updated>
    
    <content type="html"><![CDATA[<p>安定了。<br>以后在考虑建个电报channel。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;安定了。&lt;br&gt;以后在考虑建个电报channel。&lt;/p&gt;
&lt;script&gt;
        document.querySelectorAll(&#39;.github-emoji&#39;)
          .forEach(el =&gt; {
            if (!el.
      
    
    </summary>
    
    
      <category term="misc" scheme="http://blog.tensor-robotics.com/categories/misc/"/>
    
    
      <category term="misc" scheme="http://blog.tensor-robotics.com/tags/misc/"/>
    
      <category term="test" scheme="http://blog.tensor-robotics.com/tags/test/"/>
    
  </entry>
  
</feed>
