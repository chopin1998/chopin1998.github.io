<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>welcome</title>
  
  <subtitle>Tensor Robotics</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.tensor-robotics.com/"/>
  <updated>2020-04-10T02:04:14.819Z</updated>
  <id>http://blog.tensor-robotics.com/</id>
  
  <author>
    <name>chopin1998@gmail.com</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>闭门造车日记 第二篇</title>
    <link href="http://blog.tensor-robotics.com/archives/c7b825ee.html"/>
    <id>http://blog.tensor-robotics.com/archives/c7b825ee.html</id>
    <published>2020-04-08T12:21:57.000Z</published>
    <updated>2020-04-10T02:04:14.819Z</updated>
    
    <content type="html"><![CDATA[<h1 id="造车日记-驱动板规划"><a href="#造车日记-驱动板规划" class="headerlink" title="造车日记 - 驱动板规划"></a>造车日记 - 驱动板规划</h1><h2 id="总线之首选-gt-EtherCAT"><a href="#总线之首选-gt-EtherCAT" class="headerlink" title="总线之首选 -> EtherCAT"></a>总线之首选 -&gt; EtherCAT</h2><p>久闻 <a href="https://www.ethercat.org/" target="_blank" rel="noopener">EtherCAT</a>大名，想着就以帅的名义学习一番，看看能不能用作小车的内部主干总线。</p><p>经过几个晚上的学习，大概摸清了Ethercat的毛皮。</p><p>EtherCAT， 是一个以以太网为基础的工作现场总线。它完全沿用了<a href="https://en.wikipedia.org/wiki/Ethernet" target="_blank" rel="noopener">以太网</a>的物理层，并延续了标准的<a href="https://en.wikipedia.org/wiki/Ethernet_frame" target="_blank" rel="noopener">以太帧</a>，但后面的使用上则完全不同。</p><hr><h3 id="常规以太网"><a href="#常规以太网" class="headerlink" title="常规以太网"></a>常规以太网</h3><p>在常规的以太网中，并没有中心节点，每个节点都可以占用网络，并相应发展出载波监听/冲突检测技术，当多个节点同时想收发信息时，必定会有不确定的延时。</p><p>此外虽然以太网的基础速率较高，但是对于高频次的短小消息来说，整体总线利用率可以说非常低（2-5%）</p><hr><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>在一个EtherCAT网络有且只有唯一的<strong>主站</strong>，<strong>从站</strong>节点则可以多达六万多个。而网络利用率却可以达到~97%，同时，从站间的同步偏差小于1us，在包含100个伺服轴节点的系统中，可以以10kHz的速度更新它们的状态信息。</p><p>这对于一个有众多执行机构，并且需要可控同步的机器人系统来说太美好了。</p><p>基本上主站的网络设备可以使用常规的以太网卡。EtherCAT改造的是从站节点的“网卡”。</p><p>在EtherCAT中每个从节点都有一个特制的网卡，该卡有2个独立的port（也有的有3个）。其中一个连接上行设备（或者主站），另外一个连接下行设备（或者留空）。</p><p>EtherCAT子设备网卡，支持一种被称为“Processing On the Fly”的技术，当一个从站收到上游发来的数据时，立即在硬件上将属于自己的数据更新，并立刻从下游端口发出新的数据。</p><p>维基百科上的一个动图很好的解释了这一点。<br><img src="c7b825ee/EthercatOperatingPrinciple.svg" alt=""></p><hr><h3 id="网卡"><a href="#网卡" class="headerlink" title="网卡"></a>网卡</h3><p>从站的EtherCAT网络方案有大概三种</p><ul><li>SOC， ARM核心带ethercat控制器</li><li>FPGA Core</li><li><strong>收发器</strong></li></ul><p>使用SoC的芯片应该是更合理的方式，不过当前支持EtherCAT的SoC并不算多，所以我重点看了两款独立的收发器芯片，这样搭配自己的处理器会灵活一些。</p><ul><li>LAN9252</li><li>AX58100</li></ul><p>基本大同小异吧， 以AX58100为例， 内部框图如下<br><img src="c7b825ee/ax58100_block.jpg" alt=""></p><p>典型系统构成<br><img src="c7b825ee/ethercat_system.jpg" alt=""></p><p>感觉开发从站的话应该只是工作量的问题，并没有明显障碍。</p><hr><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>目前为止，一起都很美好，除了EtherCAT的性能对于一个玩具小车来说有些overkill。</p><p>直到我大略的看了一下<strong>主站方案</strong>……问题出来了。</p><p>似乎主流的Linux主站方案都停留在2013年左右，对LinuX内核支持停留在 2.6/3.x的时代，而我电脑目前的内核版本是5.5.9……</p><p>更为头疼的是因为EtherCAT的实时性非常高，似乎如果主站不是实时操作系统的话，从节点会因为得不到及时相应而罢工……</p><p>看了一圈，似乎目前的方案是引入这个项目</p><p><strong><a href="https://gitlab.denx.de/Xenomai/xenomai/-/wikis/home" target="_blank" rel="noopener">Xenomai</a></strong></p><p>这是一个让计算机运行双内核的东西。标准LinuX内核+实时内核。</p><p>这玩笑就开大了……完全搞定这个，似乎研究生都能毕业一回。而我要的只是一个能控制小车的总线……</p><hr><h3 id="最终选择"><a href="#最终选择" class="headerlink" title="最终选择"></a>最终选择</h3><p>EtherCAT性能确实优异，看上去也很好玩，再合适的场景下确实相当帅。</p><p>我会继续看下去，但是恐怕不是在小车这个东西上了。</p><p>所以最终，小车的主控（暂定RK3399）和电机板之间的总线为</p><p><strong>USB</strong></p><hr><h2 id="驱动板MCU"><a href="#驱动板MCU" class="headerlink" title="驱动板MCU"></a>驱动板MCU</h2><p>这里先透露一下 <strong><a href="https://blog.tensor-robotics.com/categories/FoC/">FoC计划</a></strong> 的首款PCB<br><img src="c7b825ee/stm32-foc-v1.jpg" alt=""></p><p>小车的话，打算采用同款MCU <strong>STM32F303RET6</strong></p><p>有四个QD，在CubeMX里试了一下，有足够的资源硬件解码。<br><img src="c7b825ee/cube_qd.jpg" alt=""></p><p>USB也靠它片上实现</p><p>此外8根PWM，驱动电机</p><p>说不定灯效也放在它身上，到时候看</p><hr><h2 id="电机驱动器"><a href="#电机驱动器" class="headerlink" title="电机驱动器"></a>电机驱动器</h2><p>作为MPS脑残粉，电源、驱动之类的肯定首选MPS。</p><p>本来看中 <strong><a href="https://www.monolithicpower.com/en/mp6519.html" target="_blank" rel="noopener">MP6519</a></strong> </p><p>这是一颗28v 5A的可控恒流源。本来想着可以对付因为感抗造成的DC启动性能差的问题，不过后经高人提醒，电机的调速用电流源的话，会非常不线性。</p><p>最后基本确定为 <strong>MP6612</strong> 这颗。</p><hr><p>不日将开工画板。</p><p>to be continued..</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;造车日记-驱动板规划&quot;&gt;&lt;a href=&quot;#造车日记-驱动板规划&quot; class=&quot;headerlink&quot; title=&quot;造车日记 - 驱动板规划&quot;&gt;&lt;/a&gt;造车日记 - 驱动板规划&lt;/h1&gt;&lt;h2 id=&quot;总线之首选-gt-EtherCAT&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
    
      <category term="造车日记" scheme="http://blog.tensor-robotics.com/categories/%E9%80%A0%E8%BD%A6%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="分享和生产知识" scheme="http://blog.tensor-robotics.com/tags/%E5%88%86%E4%BA%AB%E5%92%8C%E7%94%9F%E4%BA%A7%E7%9F%A5%E8%AF%86/"/>
    
      <category term="小车" scheme="http://blog.tensor-robotics.com/tags/%E5%B0%8F%E8%BD%A6/"/>
    
      <category term="EtherCAT" scheme="http://blog.tensor-robotics.com/tags/EtherCAT/"/>
    
  </entry>
  
  <entry>
    <title>闭门造车日记 第一篇</title>
    <link href="http://blog.tensor-robotics.com/archives/f7aec936.html"/>
    <id>http://blog.tensor-robotics.com/archives/f7aec936.html</id>
    <published>2020-03-29T06:31:39.000Z</published>
    <updated>2020-03-29T08:22:36.905Z</updated>
    
    <content type="html"><![CDATA[<h1 id="造车日记-开篇"><a href="#造车日记-开篇" class="headerlink" title="造车日记 - 开篇"></a>造车日记 - 开篇</h1><h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>看到<a href="https://www.dji.com/cn/robomaster-s1" target="_blank" rel="noopener">大疆 RoboMaster S1</a>之后，就有了自己也做一台小车的想法。</p><p>当然除了以后给女儿当玩具玩儿之外，目的还是非常明确的。</p><h3 id="在硬件层面"><a href="#在硬件层面" class="headerlink" title="在硬件层面"></a>在硬件层面</h3><p>主要是尝试新技术以及为一些技术的工程化积累经验。包含但不限于</p><ul><li>CAN/ Ethercat 等现场总线</li><li>高等级嵌入式芯片和开发</li><li>各种新传感器</li></ul><h3 id="在软件层"><a href="#在软件层" class="headerlink" title="在软件层"></a>在软件层</h3><p>有一个专一但是庞大的目标：实践<a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping" target="_blank" rel="noopener">vSLAM</a></p><p>在目前已知的室内定位技术中，总感觉 基于RGB-D的vSLAM是才王道，其它都是异端，虽然异端也有异端的作用。</p><hr><h2 id="首批硬件"><a href="#首批硬件" class="headerlink" title="首批硬件"></a>首批硬件</h2><h3 id="底盘"><a href="#底盘" class="headerlink" title="底盘"></a>底盘</h3><p>最近工作原因多少接触了一些CAD软件，本来想着自己从头画图开料，但是毕竟不是本行，时间花在这个上面不是太划算，于是就花了几个晚上逛了逛 <del>窑子</del> 不对淘宝。终于找到了还算满意的小平台。四麦克纳姆轮。</p><p><img src="f7aec936/chassis.jpg" alt=""></p><p>和老板哭了一会儿穷（真穷），饶了我￥40块，尽管如此，心还是在滴血…</p><p><img src="f7aec936/b_joint.jpg" alt=""></p><p>底部的一个设计还是挺好的，虽然买不起四轴独立悬架版本，好歹有一个低成本板车方案 ^_^</p><p>因为成本问题， 这版就先用有刷电机，等我FoC完全整明白了， 再给换装无刷电机。</p><h3 id="动力电池"><a href="#动力电池" class="headerlink" title="动力电池"></a>动力电池</h3><p><img src="f7aec936/batt.jpg" alt=""></p><p>采用了特斯拉 3系同款的 <strong>21700</strong>电芯，3s2p结构。相比比传统的18650电池</p><ul><li>价格更低</li><li>容量更高</li><li>内阻更低</li><li>一致性更好</li><li>寿命更长</li></ul><p>原打算自己做BMS的，后来想想，一共就6颗电芯，好像啥管理必要了，直接让卖家给我封了一块充放电同口的保护板进去了事。￥170块。</p><h3 id="遥控"><a href="#遥控" class="headerlink" title="遥控"></a>遥控</h3><p>第一阶段的调试，也是为了好玩，一个能直接用的手柄还是需要的。<br><img src="f7aec936/remote_ctrl.jpg" alt=""></p><p>taobao又搜了一个晚上，发现这个。乐视的手柄。虽然我对乐视完全无感，但是￥40块钱，无线，还能说啥。</p><p>然后，到手之后发现， 手感还不错。就是有一个按键不太灵光，和老板反应了，他居然说要不就退款，这个就算送给我，要不就再给我寄一个，这个也送给我。。。</p><p>那个坏的键， 我拆开看过， 其实内部开关都是好的， 就是模具有点儿问题， 中间垫个纸片就几乎完美。于是我花了￥40块买了两个手柄。</p><p>这个手柄在LinuX可以使用大部分功能，目前震动还不知道怎么搞出来，似乎不是标准协议…问题不大。</p><h3 id="主控（暂定）"><a href="#主控（暂定）" class="headerlink" title="主控（暂定）"></a>主控（暂定）</h3><p>RK3399</p><p><img src="f7aec936/rk3399.jpg" alt=""></p><p>双摄也有，倒是看看同步的情况，以及适合不适合跑slam。</p><hr><h2 id="下一步计划"><a href="#下一步计划" class="headerlink" title="下一步计划"></a>下一步计划</h2><h3 id="规划驱动板，选定互联总线，用遥控器自由控制"><a href="#规划驱动板，选定互联总线，用遥控器自由控制" class="headerlink" title="规划驱动板，选定互联总线，用遥控器自由控制"></a>规划驱动板，选定互联总线，用遥控器自由控制</h3><h3 id="开始构建和实践SLAM"><a href="#开始构建和实践SLAM" class="headerlink" title="开始构建和实践SLAM"></a>开始构建和实践SLAM</h3><h3 id="其它好玩儿的附加功能"><a href="#其它好玩儿的附加功能" class="headerlink" title="其它好玩儿的附加功能"></a>其它好玩儿的附加功能</h3><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;造车日记-开篇&quot;&gt;&lt;a href=&quot;#造车日记-开篇&quot; class=&quot;headerlink&quot; title=&quot;造车日记 - 开篇&quot;&gt;&lt;/a&gt;造车日记 - 开篇&lt;/h1&gt;&lt;h2 id=&quot;缘起&quot;&gt;&lt;a href=&quot;#缘起&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
      <category term="造车日记" scheme="http://blog.tensor-robotics.com/categories/%E9%80%A0%E8%BD%A6%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="分享和生产知识" scheme="http://blog.tensor-robotics.com/tags/%E5%88%86%E4%BA%AB%E5%92%8C%E7%94%9F%E4%BA%A7%E7%9F%A5%E8%AF%86/"/>
    
      <category term="小车" scheme="http://blog.tensor-robotics.com/tags/%E5%B0%8F%E8%BD%A6/"/>
    
  </entry>
  
  <entry>
    <title>安德猴的另类玩法</title>
    <link href="http://blog.tensor-robotics.com/archives/75e36441.html"/>
    <id>http://blog.tensor-robotics.com/archives/75e36441.html</id>
    <published>2020-03-24T12:41:59.000Z</published>
    <updated>2020-03-29T08:15:18.167Z</updated>
    
    <content type="html"><![CDATA[<p>要耍安德猴，可以</p><ul><li>在装有android的平板、手机上使用</li><li>可以用android sdk自带的模拟器（基于qemu）</li><li>还可以使用virtualbox/ vmware这样的虚拟机运行x86 android</li></ul><p>体验从上到下， 依次80分，50分和10分。（iPhone是 100分的话）</p><hr><p>本文介绍一种新的方式，将安卓置于Linux内核原生的容器（container）中，具有基本的GPU硬件加速，同时自带android的核心系统服务。</p><p>说人话，就是让android app原生的运行在一台PC中</p><p>仿佛有一台android设备运行在电脑里一样。但是并不是虚拟机或者模拟器，所以速度很快。</p><p>初步体验， 可以打到60~70分。</p><hr><p>主角： <a href="http://anbox.io" target="_blank" rel="noopener"><strong>ANBOX</strong></a></p><hr><p>官方网站的安装说明已经非常详细，这里只是大概记录一下。</p><p>首先Debian源里的并不是新版本，官方目前只支持snap包管理系统，所以如果你的Debian里还没有snap的话需要</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install snapd</span><br></pre></td></tr></tbody></table></figure><hr><p>Anbox安装</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo snap install --devmode --beta anbox</span><br></pre></td></tr></tbody></table></figure><p>如果日后需要更新</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo snap refresh --beta --devmode anbox</span><br></pre></td></tr></tbody></table></figure><p>snap在国内访问会非常不和谐，但是snap又不能直接使用环境变量， 需要修改其配置</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl edit snapd.service</span><br></pre></td></tr></tbody></table></figure><p>修改或加入下面几行</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=http_proxy=http://127.0.0.1:8123</span><br><span class="line">Environment=https_proxy=http://127.0.0.1:8123</span><br></pre></td></tr></tbody></table></figure><hr><p>Anbox内核模块安装</p><p>需要首先apt安装 dkms和内核头文件。</p><p>之后从git clone内核模块的源代码</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/anbox/anbox-modules.git</span><br></pre></td></tr></tbody></table></figure><p>编译</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp -rT ashmem /usr/src/anbox-ashmem-1</span><br><span class="line">sudo cp -rT binder /usr/src/anbox-binder-1</span><br><span class="line">sudo dkms install anbox-ashmem/1</span><br><span class="line">sudo dkms install anbox-binder/1</span><br></pre></td></tr></tbody></table></figure><p>加载测试</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo modprobe ashmem_linux</span><br><span class="line">sudo modprobe binder_linux</span><br></pre></td></tr></tbody></table></figure><p>正常情况下，lsmod应该就能看到刚刚加载的模块</p><p>/dev/下面也会出现对应的设备</p><ul><li>/dev/binder</li><li>/dev/ashmem</li></ul><p>需要修改这两个文件的访问权限， 让普通用户也能访问。</p><hr><p>激动人心的时刻， 运行</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">anbox.appmgr</span><br></pre></td></tr></tbody></table></figure><p>第一次运行的话， CPU会加载android的系统环境， 在我电脑上大概几秒吧 ，完了之后就能看到一个新窗口<br><img src="75e36441/anbox_appmgr.jpg" alt=""></p><p>点开里面原生自带的应用，每一个都会是一个独立的窗口。</p><hr><p>这时可以用adb shell连进去看看。<br><img src="75e36441/adb_shell.jpg" alt=""></p><p>可以看到android显示的内核、硬件都是真机的，这就是Linux内核容器。</p><p>不要惊讶分辨率，我用的是1440p+1080p双显示器</p><p>此时android会用以太网卡和主机连接，而在主机端呢， 刚刚的内核驱动则会虚拟出一个anbox0网络设备。</p><hr><p>不爽之处</p><ul><li>目前的img只有 7.1</li><li>似乎并不支持摄像头</li></ul><hr><p>喔，对了， 有个脚本可以给默认的android image添加上google play，安装app方便很多</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/geeks-r-us/anbox-playstore-installer.git</span><br></pre></td></tr></tbody></table></figure><p><img src="google-play.jpg" alt=""></p><hr><p>就到这里，</p><p>下次再见。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;要耍安德猴，可以&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在装有android的平板、手机上使用&lt;/li&gt;
&lt;li&gt;可以用android sdk自带的模拟器（基于qemu）&lt;/li&gt;
&lt;li&gt;还可以使用virtualbox/ vmware这样的虚拟机运行x86 android&lt;/li&gt;

      
    
    </summary>
    
    
      <category term="CS" scheme="http://blog.tensor-robotics.com/categories/CS/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="android" scheme="http://blog.tensor-robotics.com/tags/android/"/>
    
      <category term="安德猴" scheme="http://blog.tensor-robotics.com/tags/%E5%AE%89%E5%BE%B7%E7%8C%B4/"/>
    
      <category term="anbox" scheme="http://blog.tensor-robotics.com/tags/anbox/"/>
    
  </entry>
  
  <entry>
    <title>ST MCSDK 的开箱使用 | 电机系列 第二篇</title>
    <link href="http://blog.tensor-robotics.com/archives/b19fd6ba.html"/>
    <id>http://blog.tensor-robotics.com/archives/b19fd6ba.html</id>
    <published>2020-03-23T14:13:04.000Z</published>
    <updated>2020-03-29T08:11:55.329Z</updated>
    
    <content type="html"><![CDATA[<p>因为工作的关系，接触到一些电机的驱动。最早是BLDC，跟着老工程师后面，从懵懵懂懂，到稍微有了一点儿概念。之后自己憋出一个能走微步的步进电机驱动。满满的成就感记忆犹新。</p><p>后面的项目，用集成驱动芯片独立完成了一些其它的电机驱动。</p><p>前二年的学习过程中，听说了FOC这个词，似乎各种厉害各种邪乎。</p><hr><p><img src="b19fd6ba/foc_feature.jpg" alt=""></p><p>于是各种买书，各种懵逼。心潮澎湃想自己从零实现一整套，搞了一段时间，越搞坑越深，最后FOC没搞出来，传统六步方波式的驱动器倒是攒出来。不甘心啊。</p><p>也有一些优秀的厂家提供了不错的集成方案，套片解决问题，不过因为项目使用限制，虽然已经调通了demo但是并没有实际使用上。</p><p>9102年就要过去，年初制定的年度计划眼看着又有一项要完不成（为什么说”又”呢。。。淡淡的忧伤……）</p><p>突然得知ST的电机驱动库发布了重要的更新，并且全部开源（有一定限制）。抓着最后几周，调通demo。并且有继续深入下去的可能了。</p><p>////<br>本番废话略多，下面进入正题。<br>////</p><hr><p>相比六步方波，FOC驱动中包含诸如Park/Clark变换等带有大量三角函数的矩阵计算，以前的8/16单片机就有些捉襟见肘（XMEGA系列处于勉强够用的边缘）。</p><p>因此这里选择了STM32F303REt6作为主控。72Mhz 带硬件数学单元的Cortex M4算力够强，此外它内建了为高阶电机驱动优化过的定时器，特别是还集成了大量的模拟比较器，运算放大器和ADC。将来自己做PCB的话，只要配合桥驱动器和mosfet就可以。</p><p>ST 提供的MCSDK提供了FOC的底层库，以及一些实用程序。ST官网给出了两个版本，MCSDK，以及MCSDK-FULL，二者的区别是MCSDK中的部分高阶算法（比如状态观测器、前馈控制等）只以二进制库的方式提供，而full的版本则包含全部的源代码。</p><p>可能是刚刚出新的原因，目前MCSDK仅提供了m$ windows版本，只能在虚拟机里搞搞，不过ST承诺会在2020年推出重构的MCSDK，将全平台支持。</p><p><img src="b19fd6ba/linux_version.jpg" alt=""></p><hr><p>装好后，得到一些新图标</p><p><img src="b19fd6ba/icon.jpg" alt=""></p><hr><p>使用Keil/IAR等其它ide的，STM32CUBEIDE就不再需要。重要的是左边上下的两个。Motor Profiler和 MotorControl Workbench</p><p>Motor Profiler是一套很好玩儿的实用程序。它使用内置的固件（因此目前只能支持官方开发板）自动侦测电机的各种参数，以待后用。</p><p>打开它，界面如下</p><p><img src="b19fd6ba/gui.jpg" alt=""></p><hr><p>选择好手上的控制板和驱动后，需要填写电机极对数，不知道的话可以用下面的方法测定：<br>使用恒流源接入电机的任意两相，然后手动转动电机转子一圈，边转边计数有几次咯噔咯噔（术语：掣子），如果是4次，那么电机就是4对磁极。</p><p>如果使用的驱动板能力远远大过电机或者电源的额定电流，需要正确填写电机的最大电流，否则驱动板会尝试使用很大的电流测试电机。</p><p>剩下的参数，可填可不填。</p><p>完了就可以点击connect和Start Profile。<br>程序会先后测定电机的</p><ul><li>相 电阻</li><li>相 电感</li><li>反电动势常数</li><li>转动惯量</li><li>机械摩擦系数</li></ul><p>将测定参数命名后保存待用。</p><hr><p>接下来打开Motor Control Workbench这个程序， 并选择新建一个项目</p><p><img src="b19fd6ba/mc_wb.jpg" alt=""></p><p>与Motor Profiler不同，workbench是支持自定义板级的，这里还是先用开发板</p><p>配置核心在此。<br><img src="b19fd6ba/mc_wb_config.jpg" alt=""></p><p>这里可以选择底层驱动相关的各种参数，包括：</p><ul><li>母线电压相关</li><li>温度监控相关</li><li>制动器相关</li><li>电机选择（这里可以使用刚刚用Motor Profiler测量的参数，也可以- 根据电机厂商的datasheet直接填写）</li><li>驱动桥相关</li><li>电流检测配置</li><li>速度、位置检测配置</li><li>IO/模拟电路配置</li><li>等等</li></ul><hr><p>根据硬件配置好后，就可以电机生成按钮，生成配置好的初始化框架。然后点击Run Cube。进一步配置和MCSDK无关的其它CUBE配置。<br>常规的cube配置完成后，再点击gennerate code就完成的所有外设，和库的初始化代码， 以及一个空的 main() （如果没有配置freeRTOS的话）</p><p>我一开始下载的版本是 5.4.1， 配合cube的常规固件库 stm32f303 1.10.0，经常会出现一些奇奇怪怪的问题，这个不能编译，那个少个依赖什么的。</p><p>后来更新到了 5.4.3似乎好了一点儿。。但是总是很悬的感觉。因为就算是5.4.3生成出来的代码，我也是修改了一点点东西才能编译……还是期望Linux版本早点儿出来吧。。。</p><p>ST还提供了测试界面， 如果代码生产正常，就可以简单驱动起来。</p><p><img src="b19fd6ba/run.jpg" alt=""></p><p>能够正确编译以后，就可以把代码拷出虚拟机，转到Linux真机后续开发，只要不动mcsdk的结构配置，倒也不再需要虚拟机。</p><p>后续将结合FOC一起介绍MCSDK的结构和开发过程。</p><p>本番结束。<br>to be continued</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;因为工作的关系，接触到一些电机的驱动。最早是BLDC，跟着老工程师后面，从懵懵懂懂，到稍微有了一点儿概念。之后自己憋出一个能走微步的步进电机驱动。满满的成就感记忆犹新。&lt;/p&gt;
&lt;p&gt;后面的项目，用集成驱动芯片独立完成了一些其它的电机驱动。&lt;/p&gt;
&lt;p&gt;前二年的学习过程中
      
    
    </summary>
    
    
      <category term="FoC" scheme="http://blog.tensor-robotics.com/categories/FoC/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="FoC" scheme="http://blog.tensor-robotics.com/tags/FoC/"/>
    
      <category term="电机驱动" scheme="http://blog.tensor-robotics.com/tags/%E7%94%B5%E6%9C%BA%E9%A9%B1%E5%8A%A8/"/>
    
  </entry>
  
  <entry>
    <title>DIY 示波器电流探头 | 电机系列 第一篇</title>
    <link href="http://blog.tensor-robotics.com/archives/c122089a.html"/>
    <id>http://blog.tensor-robotics.com/archives/c122089a.html</id>
    <published>2020-03-23T13:18:57.000Z</published>
    <updated>2020-03-29T08:11:34.972Z</updated>
    
    <content type="html"><![CDATA[<p>最近项目需要用到电流探头， 某宝一搜，心凉半截。国产靠谱品牌，最便宜的都要￥3、4000。性能略强一些的， 都得成倍成倍的往上翻。一年可能也就用那么十次八次，也不太好意思申请购买 卍</p><p><img src="c122089a/strobe.jpg" alt=""></p><p>正巧， 前段时间有收到 电子发烧友 送测的 MAX40056 EVKIT，脑袋一拍，计上心头。</p><p><img src="c122089a/max40056.jpg" alt=""></p><hr><p>美信的MAX40056是一颗 双向电流采样放大器。共模输入上限达到接近70伏，并且内建反向过冲电压防护。提供50V/V, 20V/V, 10V/V三种增益倍率的封装。</p><p>比起一般的差分放大器，这颗芯片采用针对性的技术，可以抑制PWM信号带来的噪声干扰。</p><p>MAX40056也集成了+1.5v的精密基准源，配合+3.3v单电源使用非常简单。</p><p><img src="c122089a/max40056_block.jpg" alt=""></p><hr><p>根据内部框图， 可以看到还具有快速过流保护功能。</p><p>因此这颗芯片非常适合高阶电机驱动的电流采样方案。</p><p>一般电流采样方案使用低位电阻，好处是便宜，坏处是运放同样靠近地，噪声会比较大</p><p>稍微高级一点儿， 使用高位电阻配合高共模放大器，略贵一点儿的价格带来更干净的信号，同时有一个额外的好处是可以检测到负载的对地短路事件。</p><p>但是简单的电阻采样无论高低位都一个弊端， 在桥式结构的电路中电流采样都会存在窗口期， 需要精确配合pwm。</p><p>而使用更高级的 相电流采样 方案，则完全没有这个问题，可以在任何时段内采集电流。除了更贵一丢丢外，全是优点，没有任何缺点。</p><p>MAX40056 就可以这样用。</p><p>300kHz(-3dB)的 带宽， 刚好也和最低档次的CT探头一样规格。</p><hr><p>////<br>话不多说，直接开搞。<br>////</p><p>内部 +1.5v 电压基准。</p><p><img src="c122089a/ref_voltage.jpg" alt=""></p><p>原板的采样电阻为 R050， 再加上 MAX40056F的增益是 50V/V， 所以默认配置只能采集 +/- 0.7A的电流， 果断拆了……<br>大厂原版开发板，就是严谨。教科书级的开尔文布线。</p><p><img src="c122089a/layout.jpg" alt=""></p><p>换上小十倍的电阻， 采样量程扩大10倍。当然精度也随之下降。</p><p><img src="c122089a/resistance.jpg" alt=""></p><p>完整的采样测试电路</p><p>便利性和成品CT探头肯定要差， 精度应该不落太多。</p><p>差着100+倍的成本 (=@__@=)</p><p><img src="c122089a/circuit.jpg" alt=""></p><p>四通道示波器终于派上大用场。</p><p>要用的时候，它就在那儿，的感觉真好。<br>foc扭矩控制，单相电流终于有点儿正弦波的样子了￼￼！<br>老子等这天好久好久了￼<br>工具在手，天下我有！</p><p><img src="c122089a/foc-1/scope.jpg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近项目需要用到电流探头， 某宝一搜，心凉半截。国产靠谱品牌，最便宜的都要￥3、4000。性能略强一些的， 都得成倍成倍的往上翻。一年可能也就用那么十次八次，也不太好意思申请购买 卍&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;c122089a/strobe.jpg&quot; alt=&quot;&quot;&gt;
      
    
    </summary>
    
    
      <category term="FoC" scheme="http://blog.tensor-robotics.com/categories/FoC/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="FoC" scheme="http://blog.tensor-robotics.com/tags/FoC/"/>
    
      <category term="电机驱动" scheme="http://blog.tensor-robotics.com/tags/%E7%94%B5%E6%9C%BA%E9%A9%B1%E5%8A%A8/"/>
    
  </entry>
  
  <entry>
    <title>关于ToF 番四</title>
    <link href="http://blog.tensor-robotics.com/archives/76aee792.html"/>
    <id>http://blog.tensor-robotics.com/archives/76aee792.html</id>
    <published>2020-03-22T08:51:06.000Z</published>
    <updated>2020-03-29T08:08:41.680Z</updated>
    
    <content type="html"><![CDATA[<p>由于各种原因，和本番相关的工作被严重耽误……以至于错过大好机会</p><p><strong>痛定思痛 痛何如哉</strong></p><hr><p>本番行文后，将在另外一种形式上重启</p><hr><p>在 【关于ToF 番二】 中介绍的ToF系统，使用连续调制光照射外界，并通过多帧数据，计算得出反射光的相位偏移角度， 从而间接计算出光线行走距离。</p><p>但，这不是唯一的方式，本文介绍另外一种ToF相机的操作原理：脉冲式ToF</p><p>让我们来构建一个和连续调制方式ToF的有点儿相似但不完全相同的像素结构</p><p><img src="76aee792/pulse_tof_gate.webp" alt=""></p><ul><li>首先操作时， chg会打开，对两侧两颗电容充电</li><li>然后S0(或S1)进行开关操作，开关时机由调制模块控制</li><li>像素点开始曝光，并将光信号转变成电荷</li><li>当S0（或S1）闭合时，积分电荷就会和对应电容中和</li><li>最后读取电容中的剩余电压</li></ul><p>然后有下图</p><p><img src="76aee792/op.webp" alt=""></p><ul><li>第一行 代表受控的LED输出，正是因为这个，才被称之为脉冲式</li><li>第二行 代表实际的光线反射回波</li></ul><hr><p>LED发射的每个脉冲长度为 t， 而S0和S1的采样时长也为t，但是S0于LED同时开启，S1在S0关闭后立即打开。因此得到第三行和第四行</p><ul><li>第三行 是S0的积分</li><li>第四行 是S1的积分</li></ul><p>得到公式</p><p> d = 1/2 * C * t * (S0) / (S0 + S1)</p><p>可以看到，和连续调制ToF相比</p><ul><li>脉冲ToF的结算会更容易。当然二者基本还是在一个水平线上，一个是加减乘除，一个带三角函数</li><li>连续调制ToF的像素点因为是差分结构，对环境光噪音的抑制有天生的优势</li><li>听说脉冲式ToF的精度会略高，当然还要看各自厂家的具体优化手段</li><li>连续调制ToF在计算中，可以得到一些附结论（比如误差估计）</li></ul><hr><p>最后附上一种连续调制式ToF的像素结构图</p><p><img src="76aee792/tune.jpeg" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于各种原因，和本番相关的工作被严重耽误……以至于错过大好机会&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;痛定思痛 痛何如哉&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;本番行文后，将在另外一种形式上重启&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;在 【关于ToF 番二】 中介绍的ToF系统，使用连续调制
      
    
    </summary>
    
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/categories/ToF/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/tags/ToF/"/>
    
  </entry>
  
  <entry>
    <title>关于ToF 番三</title>
    <link href="http://blog.tensor-robotics.com/archives/3a7c8b8c.html"/>
    <id>http://blog.tensor-robotics.com/archives/3a7c8b8c.html</id>
    <published>2020-03-22T08:03:06.000Z</published>
    <updated>2020-03-29T08:08:24.960Z</updated>
    
    <content type="html"><![CDATA[<p>由于各种事物影响， ToF项目被暂缓了￼， 趁着公司再次搬家的这段时间，重新捡起来。</p><iframe frameborder="0" src="https://v.qq.com/txp/iframe/player.html?vid=o1357dpu781" allowfullscreen="true"></iframe><p>先是把代码又理了一遍，抽空学习了一下numpy， 搞一点儿opencv，tensorflow和pytorch，把第二编辑器换成了VSCode（顺便看了一下markdown和简单的Latex语法），换用Python 3.7，还有vrep， openscad，steamVR HDK什么的……咦，好像扯远了……然后继续攻目前的障碍。这把终于弄通了￼ 其实很简单的嘛。。。。</p><hr><p>非英语母语国家的厂家写的英文文档遇上了半吊子英语水平的我的结果是世界上最远的距离变成了本来往后退一步就能到达的地方却要拼命往前跑￼</p><p>是的，我看文档时就和你读上句话的感受是一样的￼</p><p><img src="3a7c8b8c/blurred.webp" alt=""><br> 这是我用mardkdown做的笔记， 数学公式真的好漂亮。有多漂亮呢？为了让人看不清楚，我特地使用了高斯模糊处理了一下</p><hr><p>前面两番已经交代了深度视觉特别是ToF技术的概况，今天来谈一谈ToF具体实现上的大坑（之一，我预感我还会面临最后一个坑）。</p><p>ToF是一把用来测量距离的尺子，造一把尺子的关注点是精度，而造一批尺子关注的是制造精度的分布。</p><hr><p>一把尺子造出来， 如何知道它是准的？有多么不准？不准了怎么办？￼</p><p>这就涉及到标定 &amp; 补偿</p><ul><li>标定，是指出厂前，使用另外一把更准的尺子教新尺子重新做人，标定后会得到一组标定数据</li><li>补偿，是在运行时，利用出厂前的标定数据，实时地对当前原始数据进行算法处理，得到最终的结果</li></ul><p>那么为什么从传感器直接得到的原始数据不能拿来用呢？我也想呀！可我们面对的是裸片，是真实的物理世界，搞不好还有量子效应。</p><p>因为NDA的缘故， 我不能说得太具体￼ 但参考各家公开的文档，基本情况大差不差。</p><p>造成ToF系统产生误差的重要原因是“热”  中医分内热和外热两种，电烙铁也是如此… 热导致电信号（电子、空穴移动）在硅基芯片内的传导时间发生变化，而ToF高度依赖时间测量的精度。 光速大约 30厘米/纳秒， 若要达到厘米级的测距精度，对时间的测量精度不能低于30皮秒级别￼</p><p>然而环境温度的变化， 芯片工作时的温升， 都会导致精度的大规模失准， 特别是像素和像素之间的温差和其它工艺因素的差别会更加劣化这种情况￼</p><p>不同厂家针对器件的特性，都有各自的办法来解决这个问题。有些厂家的的设计比较独到，为批量化自动生产打开了便利之门。</p><p>另外一个造成误差的主要因素是环境光， 虽说相位检测的ToF系统天生具有差分读取结构，但是过强的环境光或者环境光扰动会使得调制信号的信噪比降低。 </p><p>对于这种情况，补偿算法是一方面，加强调制光照明强度，减少曝光时间，以及光学滤镜都是可以尝试的手段。</p><p>当然还有一些其它的因素，有些是特定器件相关的， 有些影响相对较小，这里不再一一熬述￼</p><p>听上去有点儿复杂，实现起来细节也确实很多，但 是可行的。</p><p>我想这就是个机会，将来总会有厂家把这些算法ASIC化，或者有人给出更简单的打包方案，不过目前，我将会来完成这些￼</p><p>毕竟朕是在暴风雨里单日骑行超过400km的人， 老子的目标在很远的地方。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于各种事物影响， ToF项目被暂缓了￼， 趁着公司再次搬家的这段时间，重新捡起来。&lt;/p&gt;
&lt;iframe frameborder=&quot;0&quot; src=&quot;https://v.qq.com/txp/iframe/player.html?vid=o1357dpu781&quot; allo
      
    
    </summary>
    
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/categories/ToF/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/tags/ToF/"/>
    
  </entry>
  
  <entry>
    <title>关于ToF 番二</title>
    <link href="http://blog.tensor-robotics.com/archives/78201d81.html"/>
    <id>http://blog.tensor-robotics.com/archives/78201d81.html</id>
    <published>2020-03-22T07:19:30.000Z</published>
    <updated>2020-03-29T08:08:11.287Z</updated>
    
    <content type="html"><![CDATA[<p>书接前文。【关于ToF 番一】</p><p>上回说到ToF的基本概念， 即LED/LD对着场景发射光线，如果场景中有物体反射光线回来，那么我们测定这道光线从发射到被接收的时间间隔， 就能计算出距该离物体的长度为 1/2 * c * t， c是光速。</p><p>举个例子， 如果光线的旅行时间测得为 5ns， 取光速为30万千米每秒， 那么光线总的行走距离就是 300000000 * 0.000000005 = 1.5米， 因为是一来一回再除以二， 就知道有个物体距离接收器0.75米（光源和接收器在同一位置）</p><p><img src="78201d81/measure.png" alt=""></p><hr><p>对于一个直接ToF系统来说，精度和误差均只依赖于对时间的测量精度。</p><p>当然，由于目前的技术限制，使用直接ToF的成本相对较高，单个diode的体积也很大，一般只用于单点或者几个点的方案。对于阵列式的图像ToF系统，使用的是间接ToF方式。 即不直接测量光线往返时间， 而是发射连续的调制光（例如正玄波）， 通过测量相位偏移的方式间接得出ToF。</p><p>初始发射时从正玄波的0度角开始连续发射， 然后接收到光线时， 通过鉴相算法， 得到相位，通过这个相位差以及调制光的频率， 就能算出光线的飞行时间。 真TM高级！ 不过这也引入了一个雷达里的常用术语：maximum unambiguous range。 具体的算法以及处理，本文暂且按下不表，日后分解，下同。</p><p><img src="78201d81/phase.png" alt=""></p><hr><p>虽然降低了难度， 但是也引入更多噪音。 因为测量相移受到诸多因素的影响，例如系统时钟的ppm，传感器温度，物体反射强度，背景光线强度等。</p><p>这回我们就简单说说ToF的工作、设计流程。</p><p>从整个系统看， 首先要考虑的是测距范围问题。 因为光线强度的减弱是距离倒数的平方关系。 例如：期待的测距范围是 5cm ~ 5m， 假如在5cm时的照明输出强度是1的话， 满足5m处同样条件的输出强度得是1000。当然合理的设计光路和LED位置布局， 可以在一定程度上减少功率输出的需求。</p><p>而如果被侧物体中有大量黑体，噗… 是对红外反射率较低的黑色物体， 那么对发射功率的要求又会提高。</p><p>ToF阵列里的每个pixel， 都可以计算出它的振幅质量， 如果过低（欠曝）或者过高（过曝）都无法解析出距离信息， 需要在下一个周期中，进行曝光调整。</p><p>如果需要更高的动态范围， 可以使用各类HDR技术，例如多帧曝光、行间切换曝光等。</p><p>对于曝光时间的选择，还有另外一个问题需要考量，曝光时间越长，就会有越多的环境光被吸收， 这会伤害整体精度（环境光的变化？）。 同时过长的曝光时间， 也会造成帧率降低。 一般曝光时间不要超过1ms，或者可以使用带通滤光片过滤掉调制光频率以外的光线。</p><p>一个典型的ToF计算流程包括：</p><ul><li>多次以不同时长曝光的ToF数据采样（用于鉴相，取得距离数据）</li><li>一次最小曝光时间的灰度数据采集（用于过滤环境光线）</li><li>周期性的传感器片上温度数据读取（用于温度补偿)</li><li>根据算法取出有效数据（过滤掉过曝太多、欠曝过多或者振幅数值过小的数据集）</li><li>温度补偿算法</li><li>背景光补偿算法</li><li>系统常量校准算法</li></ul><hr><p>to be continued</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;书接前文。【关于ToF 番一】&lt;/p&gt;
&lt;p&gt;上回说到ToF的基本概念， 即LED/LD对着场景发射光线，如果场景中有物体反射光线回来，那么我们测定这道光线从发射到被接收的时间间隔， 就能计算出距该离物体的长度为 1/2 * c * t， c是光速。&lt;/p&gt;
&lt;p&gt;举个例子
      
    
    </summary>
    
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/categories/ToF/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/tags/ToF/"/>
    
  </entry>
  
  <entry>
    <title>关于ToF 番一</title>
    <link href="http://blog.tensor-robotics.com/archives/43a03328.html"/>
    <id>http://blog.tensor-robotics.com/archives/43a03328.html</id>
    <published>2020-03-22T06:32:01.000Z</published>
    <updated>2020-03-26T01:33:47.787Z</updated>
    
    <content type="html"><![CDATA[<h2 id="曾经，通过CCD-CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。"><a href="#曾经，通过CCD-CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。" class="headerlink" title="曾经，通过CCD/CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。"></a>曾经，通过CCD/CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。</h2><h2 id="现在，日新月异的传感器技术以及强大的移动计算能力，让计算机获得深度感知能力的可能性也越来越多。"><a href="#现在，日新月异的传感器技术以及强大的移动计算能力，让计算机获得深度感知能力的可能性也越来越多。" class="headerlink" title="现在，日新月异的传感器技术以及强大的移动计算能力，让计算机获得深度感知能力的可能性也越来越多。"></a>现在，日新月异的传感器技术以及强大的移动计算能力，让计算机获得深度感知能力的可能性也越来越多。</h2><h2 id="当获得一帧3D深度图景时，会收到一幅2D深度阵列，-其中的每个”pixel”都代表了该点对应物体的实际距离。闭上眼睛想象一下，那是一堆密密麻麻的点云。机器人将更容易的进行自主定位，计算机能更轻易的识别目标，也许新的测量工具，新的交互方式也会应运而生。"><a href="#当获得一帧3D深度图景时，会收到一幅2D深度阵列，-其中的每个”pixel”都代表了该点对应物体的实际距离。闭上眼睛想象一下，那是一堆密密麻麻的点云。机器人将更容易的进行自主定位，计算机能更轻易的识别目标，也许新的测量工具，新的交互方式也会应运而生。" class="headerlink" title="当获得一帧3D深度图景时，会收到一幅2D深度阵列， 其中的每个”pixel”都代表了该点对应物体的实际距离。闭上眼睛想象一下，那是一堆密密麻麻的点云。机器人将更容易的进行自主定位，计算机能更轻易的识别目标，也许新的测量工具，新的交互方式也会应运而生。"></a>当获得一帧3D深度图景时，会收到一幅2D深度阵列， 其中的每个”pixel”都代表了该点对应物体的实际距离。闭上眼睛想象一下，那是一堆密密麻麻的点云。机器人将更容易的进行自主定位，计算机能更轻易的识别目标，也许新的测量工具，新的交互方式也会应运而生。</h2><h2 id="双眼（和大脑）是人类获得深度感知最自然的渠道。通过长在不同地方的两只眼睛￼，视网膜上被投影了两帧“略有差异”的倒影，大脑根据几十年来的习得，分析出这两个影像中相同的物体，以及它们距离我们自身的大致距离。"><a href="#双眼（和大脑）是人类获得深度感知最自然的渠道。通过长在不同地方的两只眼睛￼，视网膜上被投影了两帧“略有差异”的倒影，大脑根据几十年来的习得，分析出这两个影像中相同的物体，以及它们距离我们自身的大致距离。" class="headerlink" title="双眼（和大脑）是人类获得深度感知最自然的渠道。通过长在不同地方的两只眼睛￼，视网膜上被投影了两帧“略有差异”的倒影，大脑根据几十年来的习得，分析出这两个影像中相同的物体，以及它们距离我们自身的大致距离。"></a>双眼（和大脑）是人类获得深度感知最自然的渠道。通过长在不同地方的两只眼睛￼，视网膜上被投影了两帧“略有差异”的倒影，大脑根据几十年来的习得，分析出这两个影像中相同的物体，以及它们距离我们自身的大致距离。</h2><h2 id="这个原理也是计算机目前深度视觉原理的三大流派之一：-双目视觉。-另外两个技术流，一个基于ToF测距技术，-另外一个基于Structured-Light技术。当然这是主要的分类，具体场景，具体产品，可能会相互融合，或者辅以其它技术、算法。"><a href="#这个原理也是计算机目前深度视觉原理的三大流派之一：-双目视觉。-另外两个技术流，一个基于ToF测距技术，-另外一个基于Structured-Light技术。当然这是主要的分类，具体场景，具体产品，可能会相互融合，或者辅以其它技术、算法。" class="headerlink" title="这个原理也是计算机目前深度视觉原理的三大流派之一： 双目视觉。 另外两个技术流，一个基于ToF测距技术， 另外一个基于Structured Light技术。当然这是主要的分类，具体场景，具体产品，可能会相互融合，或者辅以其它技术、算法。"></a>这个原理也是计算机目前深度视觉原理的三大流派之一： 双目视觉。 另外两个技术流，一个基于ToF测距技术， 另外一个基于Structured Light技术。当然这是主要的分类，具体场景，具体产品，可能会相互融合，或者辅以其它技术、算法。</h2><h2 id="此番将大致比较一下这三个技术流的主要特征。-下一更将重点介绍本屌目前正在玩儿的ToF方案原理。"><a href="#此番将大致比较一下这三个技术流的主要特征。-下一更将重点介绍本屌目前正在玩儿的ToF方案原理。" class="headerlink" title="此番将大致比较一下这三个技术流的主要特征。 下一更将重点介绍本屌目前正在玩儿的ToF方案原理。"></a>此番将大致比较一下这三个技术流的主要特征。 下一更将重点介绍本屌目前正在玩儿的ToF方案原理。</h2><hr><h3 id="就双目视觉来说，-对前端硬件特殊性的要求很低，两个普通摄像头即可（或许有同步采样要求）。但要从两张平面图像恢复出原本的深度信息，重建工作对于算力、算法的要求可是相当相当的高。此外对于弱光环境、光线扰动、无特征的面（一堵白墙）都无法很好处理。其速度响应、帧率、可靠性都会多少有问题。至于它的优点嘛。。除了刚刚说的硬件比较便宜外，还有一个就是分辨率理论上可以做得比较高（受限于算力）。"><a href="#就双目视觉来说，-对前端硬件特殊性的要求很低，两个普通摄像头即可（或许有同步采样要求）。但要从两张平面图像恢复出原本的深度信息，重建工作对于算力、算法的要求可是相当相当的高。此外对于弱光环境、光线扰动、无特征的面（一堵白墙）都无法很好处理。其速度响应、帧率、可靠性都会多少有问题。至于它的优点嘛。。除了刚刚说的硬件比较便宜外，还有一个就是分辨率理论上可以做得比较高（受限于算力）。" class="headerlink" title="就双目视觉来说， 对前端硬件特殊性的要求很低，两个普通摄像头即可（或许有同步采样要求）。但要从两张平面图像恢复出原本的深度信息，重建工作对于算力、算法的要求可是相当相当的高。此外对于弱光环境、光线扰动、无特征的面（一堵白墙）都无法很好处理。其速度响应、帧率、可靠性都会多少有问题。至于它的优点嘛。。除了刚刚说的硬件比较便宜外，还有一个就是分辨率理论上可以做得比较高（受限于算力）。"></a>就双目视觉来说， 对前端硬件特殊性的要求很低，两个普通摄像头即可（或许有同步采样要求）。但要从两张平面图像恢复出原本的深度信息，重建工作对于算力、算法的要求可是相当相当的高。此外对于弱光环境、光线扰动、无特征的面（一堵白墙）都无法很好处理。其速度响应、帧率、可靠性都会多少有问题。至于它的优点嘛。。除了刚刚说的硬件比较便宜外，还有一个就是分辨率理论上可以做得比较高（受限于算力）。</h3><h3 id="而Structured-Light呢，翻译过来是“结构光”。-是一种很好玩儿的技术。-最新的Apple-iPhone-X应该就是这个技术的一个变种。"><a href="#而Structured-Light呢，翻译过来是“结构光”。-是一种很好玩儿的技术。-最新的Apple-iPhone-X应该就是这个技术的一个变种。" class="headerlink" title="而Structured Light呢，翻译过来是“结构光”。 是一种很好玩儿的技术。 最新的Apple iPhone X应该就是这个技术的一个变种。"></a>而Structured Light呢，翻译过来是“结构光”。 是一种很好玩儿的技术。 最新的Apple iPhone X应该就是这个技术的一个变种。</h3><h3 id="基于结构光技术的深度系统，有一个主动（红外）光源，像外界投射设计过的点阵，或者干涉条纹或者别的什么pattern。这些pattern光遇到物体反射回摄像头。后端算法会根据这些点在摄像头里的位置推出物体实际的距离。"><a href="#基于结构光技术的深度系统，有一个主动（红外）光源，像外界投射设计过的点阵，或者干涉条纹或者别的什么pattern。这些pattern光遇到物体反射回摄像头。后端算法会根据这些点在摄像头里的位置推出物体实际的距离。" class="headerlink" title="基于结构光技术的深度系统，有一个主动（红外）光源，像外界投射设计过的点阵，或者干涉条纹或者别的什么pattern。这些pattern光遇到物体反射回摄像头。后端算法会根据这些点在摄像头里的位置推出物体实际的距离。"></a>基于结构光技术的深度系统，有一个主动（红外）光源，像外界投射设计过的点阵，或者干涉条纹或者别的什么pattern。这些pattern光遇到物体反射回摄像头。后端算法会根据这些点在摄像头里的位置推出物体实际的距离。</h3><h3 id="结构光技术对算法、算力的要求比起双面视觉来说下降很多。因为是主动投射，对弱光环境下的效果显著提升。但因为需要投影pattern，对超强光下的应用反而是受限的。此外结构光技术的深度系统，只能测定极短-中短距离的物体，搞不远。。。分辨精度在mm-cm级别。"><a href="#结构光技术对算法、算力的要求比起双面视觉来说下降很多。因为是主动投射，对弱光环境下的效果显著提升。但因为需要投影pattern，对超强光下的应用反而是受限的。此外结构光技术的深度系统，只能测定极短-中短距离的物体，搞不远。。。分辨精度在mm-cm级别。" class="headerlink" title="结构光技术对算法、算力的要求比起双面视觉来说下降很多。因为是主动投射，对弱光环境下的效果显著提升。但因为需要投影pattern，对超强光下的应用反而是受限的。此外结构光技术的深度系统，只能测定极短-中短距离的物体，搞不远。。。分辨精度在mm-cm级别。"></a>结构光技术对算法、算力的要求比起双面视觉来说下降很多。因为是主动投射，对弱光环境下的效果显著提升。但因为需要投影pattern，对超强光下的应用反而是受限的。此外结构光技术的深度系统，只能测定极短-中短距离的物体，搞不远。。。分辨精度在mm-cm级别。</h3><h3 id="intel的realsense和老款m-Kinect是代表产品。"><a href="#intel的realsense和老款m-Kinect是代表产品。" class="headerlink" title="intel的realsense和老款m$ Kinect是代表产品。"></a>intel的realsense和老款m$ Kinect是代表产品。</h3><h3 id="传统的固定pattern投影技术，深度分辨率也受到pattern发生器的限制。"><a href="#传统的固定pattern投影技术，深度分辨率也受到pattern发生器的限制。" class="headerlink" title="传统的固定pattern投影技术，深度分辨率也受到pattern发生器的限制。"></a>传统的固定pattern投影技术，深度分辨率也受到pattern发生器的限制。</h3><h3 id="苹果那个就比较牛逼，传说pattern是可变的，分辨精度在极短距离内可以达到惊人的um级别，分辨率在适当的范围内也相对喜人。"><a href="#苹果那个就比较牛逼，传说pattern是可变的，分辨精度在极短距离内可以达到惊人的um级别，分辨率在适当的范围内也相对喜人。" class="headerlink" title="苹果那个就比较牛逼，传说pattern是可变的，分辨精度在极短距离内可以达到惊人的um级别，分辨率在适当的范围内也相对喜人。"></a>苹果那个就比较牛逼，传说pattern是可变的，分辨精度在极短距离内可以达到惊人的um级别，分辨率在适当的范围内也相对喜人。</h3><h3 id="ToF的全称是”Time-of-Flight”。本意是通过测量光线的折返时间来算出距离。我们都知道-距离-速度-×-时间。光速爱因斯坦告诉我们不变了，现在只要测定光线从物体反射回来的时间就能很容易算出距离来了。听上去是不是很酷！要测定一秒绕地球七圈的光，行走很短距离的时间？具体原理和操作，下一弹说￼。"><a href="#ToF的全称是”Time-of-Flight”。本意是通过测量光线的折返时间来算出距离。我们都知道-距离-速度-×-时间。光速爱因斯坦告诉我们不变了，现在只要测定光线从物体反射回来的时间就能很容易算出距离来了。听上去是不是很酷！要测定一秒绕地球七圈的光，行走很短距离的时间？具体原理和操作，下一弹说￼。" class="headerlink" title="ToF的全称是”Time of Flight”。本意是通过测量光线的折返时间来算出距离。我们都知道 距离 = 速度 × 时间。光速爱因斯坦告诉我们不变了，现在只要测定光线从物体反射回来的时间就能很容易算出距离来了。听上去是不是很酷！要测定一秒绕地球七圈的光，行走很短距离的时间？具体原理和操作，下一弹说￼。"></a>ToF的全称是”Time of Flight”。本意是通过测量光线的折返时间来算出距离。我们都知道 距离 = 速度 × 时间。光速爱因斯坦告诉我们不变了，现在只要测定光线从物体反射回来的时间就能很容易算出距离来了。听上去是不是很酷！要测定一秒绕地球七圈的光，行走很短距离的时间？具体原理和操作，下一弹说￼。</h3><h3 id="和结构光一样，ToF同样需要主动发射光线。当然不是“结构”的，它只是亮和暗，视界内全面覆盖。所以功耗会比结构光的大一丢丢，特别是远距离的时候，喔，对了反正结构光也搞不远…"><a href="#和结构光一样，ToF同样需要主动发射光线。当然不是“结构”的，它只是亮和暗，视界内全面覆盖。所以功耗会比结构光的大一丢丢，特别是远距离的时候，喔，对了反正结构光也搞不远…" class="headerlink" title="和结构光一样，ToF同样需要主动发射光线。当然不是“结构”的，它只是亮和暗，视界内全面覆盖。所以功耗会比结构光的大一丢丢，特别是远距离的时候，喔，对了反正结构光也搞不远…"></a>和结构光一样，ToF同样需要主动发射光线。当然不是“结构”的，它只是亮和暗，视界内全面覆盖。所以功耗会比结构光的大一丢丢，特别是远距离的时候，喔，对了反正结构光也搞不远…</h3><h3 id="新款m-Kinect是这个架构的代表。"><a href="#新款m-Kinect是这个架构的代表。" class="headerlink" title="新款m$ Kinect是这个架构的代表。"></a>新款m$ Kinect是这个架构的代表。</h3><h3 id="ToF技术架构的优势之一就是计算相对最简单，几乎是传感器直接测量。帧率高并因此响应极快。抗外界光线干扰能力很强。长、短距离皆可应用。分辨精度在mm～cm级别之间。"><a href="#ToF技术架构的优势之一就是计算相对最简单，几乎是传感器直接测量。帧率高并因此响应极快。抗外界光线干扰能力很强。长、短距离皆可应用。分辨精度在mm～cm级别之间。" class="headerlink" title="ToF技术架构的优势之一就是计算相对最简单，几乎是传感器直接测量。帧率高并因此响应极快。抗外界光线干扰能力很强。长、短距离皆可应用。分辨精度在mm～cm级别之间。"></a>ToF技术架构的优势之一就是计算相对最简单，几乎是传感器直接测量。帧率高并因此响应极快。抗外界光线干扰能力很强。长、短距离皆可应用。分辨精度在mm～cm级别之间。</h3><h3 id="缺陷除了刚刚提到的功耗外，分辨率是最大硬伤。目前-320x240算旗舰，640x480等更高规格器件或许在路线图上"><a href="#缺陷除了刚刚提到的功耗外，分辨率是最大硬伤。目前-320x240算旗舰，640x480等更高规格器件或许在路线图上" class="headerlink" title="缺陷除了刚刚提到的功耗外，分辨率是最大硬伤。目前~320x240算旗舰，640x480等更高规格器件或许在路线图上"></a>缺陷除了刚刚提到的功耗外，分辨率是最大硬伤。目前~320x240算旗舰，640x480等更高规格器件或许在路线图上</h3><h3 id="未来的道路在哪里？谁知道呢。。。走着瞧吧。。。"><a href="#未来的道路在哪里？谁知道呢。。。走着瞧吧。。。" class="headerlink" title="未来的道路在哪里？谁知道呢。。。走着瞧吧。。。"></a>未来的道路在哪里？谁知道呢。。。走着瞧吧。。。</h3><h3 id="客户需求-成本控制-正确的技术路线"><a href="#客户需求-成本控制-正确的技术路线" class="headerlink" title="客户需求 + 成本控制 + 正确的技术路线"></a>客户需求 + 成本控制 + 正确的技术路线</h3><h3 id="听说把握好这三点，就能实现财务自由￼￼￼￼"><a href="#听说把握好这三点，就能实现财务自由￼￼￼￼" class="headerlink" title="听说把握好这三点，就能实现财务自由￼￼￼￼"></a>听说把握好这三点，就能实现财务自由￼￼￼￼</h3><h3 id="而我呢，目前能抓住一个半就好￼"><a href="#而我呢，目前能抓住一个半就好￼" class="headerlink" title="而我呢，目前能抓住一个半就好￼"></a>而我呢，目前能抓住一个半就好￼</h3><h3 id="to-be-continued"><a href="#to-be-continued" class="headerlink" title="to be continued"></a>to be continued</h3><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;曾经，通过CCD-CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。&quot;&gt;&lt;a href=&quot;#曾经，通过CCD-CMOS等图像阵列传感器以及光学镜头，可以获取一帧帧平面2D的图像。&quot; class=&quot;headerlink&quot; title=&quot;曾经，通过C
      
    
    </summary>
    
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/categories/ToF/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="ToF" scheme="http://blog.tensor-robotics.com/tags/ToF/"/>
    
  </entry>
  
  <entry>
    <title>Python ctypes的使用实例</title>
    <link href="http://blog.tensor-robotics.com/archives/b396a14c.html"/>
    <id>http://blog.tensor-robotics.com/archives/b396a14c.html</id>
    <published>2020-03-22T03:17:56.000Z</published>
    <updated>2020-03-29T08:13:50.220Z</updated>
    
    <content type="html"><![CDATA[<p>Python中可以使用ctypes模块方便的调用C代码写的库， 而无须对C代码有任何多余的要求。</p><p>此仅列一例，权做记录。</p><hr><p><img src="b396a14c/ls.png" alt=""></p><p>首先有两个文件， C和对应的头文件。</p><p>在people.c中， 有三个非常简单的函数。 供日后调用。<br>而在people.h中， 是对应的函数声明， 在此不表。</p><p><img src="b396a14c/source.png" alt=""></p><hr><p>接下来的一步， 将这个C代码编译成库， 使用如下的命令<br><img src="b396a14c/compile.png" alt=""></p><p>这将生成一个叫 libpeople.so的库， 和平常编译库的方式完全一样。</p><hr><p>最后， 激动人心的时刻—-直接在python中调用这个C库。<br><img src="b396a14c/call.png" alt=""></p><p>进入ipython后， 首当其冲import ctypes， 然后用cdll里的LoadLibrary导入刚刚生成的库。</p><p>因为make_people使用了自定义的struct和指针， 所以需要现在Python里先建立对于的数据结构。</p><p>之后就能随心所欲的调用c库里的函数了。</p><hr><p>一些说明。<br>库导入后本身没有动态性， 也就是说在ipython里， 输入完mylib.之后，按tab键是不能自动补全所有的function的，但是手工输入一次之后， 还是能看到已经有的东西。</p><p>因为Python的脚本性， 如果C库没有做好类型检查，特别是指针相关的，而发生致命错误的，在Python动态运行时， Python解析器进程会崩溃。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Python中可以使用ctypes模块方便的调用C代码写的库， 而无须对C代码有任何多余的要求。&lt;/p&gt;
&lt;p&gt;此仅列一例，权做记录。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;b396a14c/ls.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;首先有两个文件， C和对应的头
      
    
    </summary>
    
    
      <category term="CS" scheme="http://blog.tensor-robotics.com/categories/CS/"/>
    
    
      <category term="生产和分享知识" scheme="http://blog.tensor-robotics.com/tags/%E7%94%9F%E4%BA%A7%E5%92%8C%E5%88%86%E4%BA%AB%E7%9F%A5%E8%AF%86/"/>
    
      <category term="Python" scheme="http://blog.tensor-robotics.com/tags/Python/"/>
    
      <category term="ctypes" scheme="http://blog.tensor-robotics.com/tags/ctypes/"/>
    
  </entry>
  
  <entry>
    <title>the first blog</title>
    <link href="http://blog.tensor-robotics.com/archives/ef0a297a.html"/>
    <id>http://blog.tensor-robotics.com/archives/ef0a297a.html</id>
    <published>2020-03-19T12:00:01.000Z</published>
    <updated>2020-04-08T09:27:23.557Z</updated>
    
    <content type="html"><![CDATA[<p>安定了。<br>以后在考虑建个电报channel。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;安定了。&lt;br&gt;以后在考虑建个电报channel。&lt;/p&gt;
&lt;script&gt;
        document.querySelectorAll(&#39;.github-emoji&#39;)
          .forEach(el =&gt; {
            if (!el.
      
    
    </summary>
    
    
      <category term="misc" scheme="http://blog.tensor-robotics.com/categories/misc/"/>
    
    
      <category term="misc" scheme="http://blog.tensor-robotics.com/tags/misc/"/>
    
      <category term="test" scheme="http://blog.tensor-robotics.com/tags/test/"/>
    
  </entry>
  
</feed>
